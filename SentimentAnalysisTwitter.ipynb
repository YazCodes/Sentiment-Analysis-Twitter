{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisTwitter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0hPqnxlRwBRDubJoiCwzH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YazCodes/Sentiment-Analysis-Twitter/blob/main/SentimentAnalysisTwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl0-4Gj-LJqE",
        "outputId": "7b4610fb-d131-4463-fdd1-7063c2a54938"
      },
      "source": [
        "# import libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# libraries for data preprocessing\r\n",
        "import nltk\r\n",
        "# download modules available with NLTK\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "from nltk.tokenize import TweetTokenizer #text preprocessing\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "# libraries for data split and feature extraction\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "# library for evaluation\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "# libraries for ML algorithms for the experiment \r\n",
        "from sklearn import svm\r\n",
        "from sklearn import tree\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "\r\n",
        "# libraries for data plotting\r\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "7SMf5GsFM7z5",
        "outputId": "76951861-cfb7-404d-d5eb-4ad7378628c8"
      },
      "source": [
        "#uploading the dataset\r\n",
        "# read csv file into a dataframe\r\n",
        "df = pd.read_csv('/content/train.csv')\r\n",
        "\r\n",
        "# find out the number of entires \r\n",
        "print(f'Number of entries: {len(df)}')\r\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of entries: 16363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6e0c6d75b1</td>\n",
              "      <td>2am feedings for the baby are fun when he is a...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id                                               Text Sentiment\n",
              "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!  negative\n",
              "1  088c60f138                          my boss is bullying me...  negative\n",
              "2  9642c003ef                     what interview! leave me alone  negative\n",
              "3  358bd9e861   Sons of ****, why couldn`t they put them on t...  negative\n",
              "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "cjyNjWYxNafi",
        "outputId": "76fdc92b-9658-40df-f715-a8bac8a540b8"
      },
      "source": [
        "#Analysising the data - Compare the amount of postitive and negative tweets \r\n",
        "temp_df = df[['Text', 'Sentiment']] #Using the text and sentiment colomn to plot the graph\r\n",
        "temp_df = temp_df.groupby('Sentiment').count() #count the number of diffrent elements in the sentiment column \r\n",
        "temp_df.plot.bar() #plot the bar chart "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0686f59fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_23hi3D8O-YA",
        "outputId": "8bbb2e0d-1f99-4778-c00b-c6aeb0c5b91b"
      },
      "source": [
        "#Data cleaning  - Important step as the ML model and data processing cannot start if the dataset contains missing values\r\n",
        "\r\n",
        "df.isnull().values.any() #This shows if there are any missing values in the dataset. True = yes false = No\r\n",
        "\r\n",
        "df = df.dropna() #drop any rows with missing values - just in case \r\n",
        "\r\n",
        "print(f'Number of entries: {len(df)}') #check if the number of entries have changed"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of entries: 16363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlLLvcgXORMc"
      },
      "source": [
        "**The results of inital data analysis** \r\n",
        "The dataset contains 16363 entires consisting of three columns. \r\n",
        "\r\n",
        "\r\n",
        "1.   ID - The Id number of each text \r\n",
        "2.   Text - The tweet in text format \r\n",
        "3.   Sentiment - Indicating if the text is postive or negative\r\n",
        "\r\n",
        "\r\n",
        "The bar charts show the data set contains under 8000 negative tweets and about 8000 postive tweets. So nearly an equal split. \r\n",
        "\r\n",
        "During data cleaning there was no missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "s2P2BqK8OQDS",
        "outputId": "62622e70-4cee-42f0-cec6-f33014e9f6cf"
      },
      "source": [
        "#Data preprocessing - This transforms the raw data into an understantable format \r\n",
        "\r\n",
        "#Tokenisation - This is the process in NLP of splitting up text into smaller individual words called Tokens\r\n",
        "#Tokenisation is important as large amounts of text data needs to be broken down into smaller units of text so our ML models can understand. \r\n",
        "\r\n",
        "#Text preprocessing - cleaning and preparing text data\r\n",
        "#Selecting a tokenizer model. -  word_tokenize - Tokenizer for normal text\r\n",
        "\r\n",
        "#TweetTokenizer - Tokenizer for tweets - As the dataset contains text in the form of tweets we will be using this . \r\n",
        "\r\n",
        "list_tokenised_text = []  #Creat an empty list\r\n",
        "\r\n",
        "for text in df['Text']: # Converting text to tokens by iterating through each row in 'Text' columm. \r\n",
        "  tokenized_text = TweetTokenizer(text)\r\n",
        "  list_tokenised_text.append(tokenized_text) #Adding the tokenzie text inside the empty list \r\n",
        "\r\n",
        "print(list_tokenised_text[:2]) #the list containing the tokenzied text \r\n",
        "\r\n",
        "#After tokenizing we need to put them back into sentences \r\n",
        "\r\n",
        "list_tokenised_sentences = []\r\n",
        "for tokens in list_tokenised_text:\r\n",
        "  text = ' '.join(tokens)\r\n",
        "  list_tokenised_sentences.append(text)\r\n",
        "\r\n",
        "# adding a  new column to data frame\r\n",
        "df['Tokenised_Text'] = list_tokenised_sentences\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<nltk.tokenize.casual.TweetTokenizer object at 0x7f0685e03710>, <nltk.tokenize.casual.TweetTokenizer object at 0x7f0685e03748>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8d2e774c5d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mlist_tokenised_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_tokenised_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mlist_tokenised_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
          ]
        }
      ]
    }
  ]
}